{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c2d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html#coco-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://cocodataset.org/#download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/albumentations-team/albumentations?tab=readme-ov-file#spatial-level-transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://albumentations.ai/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd72fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daef6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use FiftyOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098cb984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FiftyOne is an open-source tool facilitating visualization and \n",
    "#access to COCO data resources and serves as an evaluation tool for model analysis on COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408581d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239cfad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to 'C:\\Users\\krish\\fiftyone\\coco-2017\\train' if necessary\n",
      "Found annotations at 'C:\\Users\\krish\\fiftyone\\coco-2017\\raw\\instances_train2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "Loading existing dataset 'coco-2017-train'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Downloading split 'validation' to 'C:\\Users\\krish\\fiftyone\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'C:\\Users\\krish\\fiftyone\\coco-2017\\raw\\instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Downloading split 'test' to 'C:\\Users\\krish\\fiftyone\\coco-2017\\test' if necessary\n",
      "Found test info at 'C:\\Users\\krish\\fiftyone\\coco-2017\\raw\\image_info_test2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'test' is sufficient\n",
      "Loading existing dataset 'coco-2017-test'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "train_dataset = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "validation_dataset = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"validation\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_dataset = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"test\",\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9caf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.info['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fabc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in train_dataset.info['categories']:\n",
    "    if i['name'] is not None:\n",
    "        count += 1\n",
    "        \n",
    "print(f\"Number of unique classes: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97366b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8af9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few images without annotations\n",
    "for i in train_dataset[:1]:  \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1984aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The information on each image block of the train_dataset looks like.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "def visualize_sample(image_path, detections, ax):\n",
    "    # Open image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    width, height = image.size\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for detection in detections:\n",
    "        label = detection.label\n",
    "        bbox = detection.bounding_box\n",
    "        xmin = bbox[0] * width\n",
    "        ymin = bbox[1] * height\n",
    "        xmax = xmin + bbox[2] * width\n",
    "        ymax = ymin + bbox[3] * height\n",
    "        rect = patches.Rectangle((xmin, ymin), bbox[2] * width, bbox[3] * height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(xmin, ymin, label, color='white', fontsize=12, bbox=dict(facecolor='black', alpha=0.5))\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "def visualize_batch(dataset, num_images):\n",
    "    # Set up the plot\n",
    "    fig, axes = plt.subplots(num_images, 1, figsize=(20, num_images * 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Visualize each image in the dataset\n",
    "    for i, sample in enumerate(dataset[:num_images]):\n",
    "        image_path = sample.filepath\n",
    "        detections = sample.ground_truth.detections\n",
    "        visualize_sample(image_path, detections, axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a batch of images from the original dataset loader\n",
    "visualize_batch(train_dataset, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract dimensions of all images in the dataset\n",
    "# image_dimensions = []\n",
    "# for i in train_dataset:\n",
    "#     width = i.metadata.width\n",
    "#     height = i.metadata.height\n",
    "#     image_dimensions.append((width, height))\n",
    "\n",
    "# # Print unique dimensions\n",
    "# print(f\"Number of unique dimensions: {len(set(image_dimensions))}\")\n",
    "# print(f\"Unique dimensions: {set(image_dimensions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd55d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of unique dimensions: 2477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We do have a wide variety of images with different dimensions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For transformation I will go with albumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0aae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a90663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42193875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Declare an augmentation pipeline with spatial-level transforms \n",
    "# transform = A.Compose([\n",
    "#     A.Resize(width=640, height=640),  # Resize the image to 640x640\n",
    "#     A.HorizontalFlip(p=0.5),  # Flip the image horizontally with a 50% probability\n",
    "#     A.RandomRotate90(p=0.5),  # Rotate the image by 90 degrees with a 50% probability\n",
    "#     A.RandomBrightnessContrast(p=0.2),  # Randomly change the brightness and contrast\n",
    "#     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize the image\n",
    "#     ToTensorV2()  # Convert the image to a PyTorch tensor\n",
    "# ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78344c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~for next phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dddd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Faster R-CNN model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514344e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from category IDs to names\n",
    "categories = train_dataset.info['categories']\n",
    "id_to_name = {category['id']: category['name'] for category in categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9171af",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What was happening is like total classes are 80, but the 'id' for some classes are skipped..\n",
    "#Because of that the model can predict wrong class labels.\n",
    "#To overcome that I created dictionary with labels and there respective 'id' directly from 'info['categories']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c277c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480719c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random subset of 100 samples to add predictions to\n",
    "predictions_view = train_dataset.take(10, seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce460b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rather than taking the whole dataset into consideration, we will work with random 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f7829",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add predictions to samples\n",
    "with fo.ProgressBar() as progress_bar:\n",
    "    for data_sample in progress_bar(predictions_view):\n",
    "        # Load image\n",
    "        img = Image.open(data_sample.filepath).convert(\"RGB\")\n",
    "        img_tensor = T.to_tensor(img).to(device)\n",
    "        channels, height, width = img_tensor.shape\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            predictions = model([img_tensor])[0]\n",
    "       \n",
    "        pred_labels = predictions[\"labels\"].cpu().detach().numpy()\n",
    "        pred_scores = predictions[\"scores\"].cpu().detach().numpy()\n",
    "        pred_boxes = predictions[\"boxes\"].cpu().detach().numpy()\n",
    "        \n",
    "        # Convert detections to FiftyOne format\n",
    "        detections = []\n",
    "        for pred_label, pred_score, pred_box in zip(pred_labels, pred_scores, pred_boxes):\n",
    "            \n",
    "            # Get class name from ID\n",
    "            class_name = id_to_name.get(pred_label, \"unknown\")\n",
    "            \n",
    "            # Convert to [top-left-x, top-left-y, width, height]\n",
    "            # in relative coordinates in [0, 1] x [0, 1]\n",
    "            x1, y1, x2, y2 = pred_box\n",
    "            relative_box = [x1 / width, y1 / height, (x2 - x1) / width, (y2 - y1) / height]\n",
    "\n",
    "            if pred_score >= 0.80:\n",
    "                detections.append(fo.Detection(\n",
    "                    label=class_name,\n",
    "                    bounding_box=relative_box,\n",
    "                    confidence=pred_score\n",
    "                ))\n",
    "        \n",
    "        # Save predictions to dataset\n",
    "        data_sample[\"faster_rcnn\"] = fo.Detections(detections=detections)\n",
    "        data_sample.save()\n",
    "\n",
    "print(\"Finished adding predictions\")\n",
    "\n",
    "\n",
    "# Launch the FiftyOne app to visualize the dataset\n",
    "session = fo.launch_app(predictions_view)\n",
    "\n",
    "# You can block the code execution to keep the FiftyOne app running\n",
    "session.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc08864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It will only show the predictions that are greater than .80 confidence rate.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_view now is like a new dataset with all the predictions,!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in predictions_view[:1]:  \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc5025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can look above what all changes are there on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88117465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluation_results = fo.evaluate_detections(\n",
    "    predictions_view,\n",
    "    \"faster_rcnn\",\n",
    "    eval_key=\"eval\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    iou=0.5,  # IoU threshold\n",
    "    compute_mAP=True,  # Compute mAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 10 most common classes in the dataset\n",
    "counts = predictions_view.count_values(\"ground_truth.detections.label\")\n",
    "classes = sorted(counts, key=counts.get, reverse=True)[:10]\n",
    "\n",
    "# Print a classification report for the top-10 classes\n",
    "evaluation_results.print_report(classes=classes)\n",
    "\n",
    "# Print some statistics about the total TP/FP/FN counts\n",
    "print(\"TP: %d\" % predictions_view.sum(\"eval_tp\"))\n",
    "print(\"FP: %d\" % predictions_view.sum(\"eval_fp\"))\n",
    "print(\"FN: %d\" % predictions_view.sum(\"eval_fn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The IoU threshold in the evaluation function helps determine the accuracy of the predictions by setting a minimum overlap \n",
    "#requirement between the predicted and ground truth bounding boxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_results.mAP())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc46f9",
   "metadata": {},
   "source": [
    "**The computed mAP value of approximately 42.04% suggests that the model is doing a decent job at detecting and classifying objects. This is a good starting point, meaning that while the model works fairly well, there is still room for improvement to achieve better accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b5bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to evaluation patches\n",
    "eval_patches = predictions_view.to_evaluation_patches(\"eval\")\n",
    "print(eval_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9198b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For confusion metrix.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of class names using a for loop\n",
    "categories = train_dataset.info['categories']\n",
    "classes = []\n",
    "for category in categories:\n",
    "    if category['supercategory'] in ['vehicle', 'person', 'animal']:\n",
    "        classes.append(category['name'])\n",
    "\n",
    "# Print the list of class names\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrix data\n",
    "confusion_matrix = evaluation_results.confusion_matrix(classes=classes)\n",
    "\n",
    "# # Extract the confusion matrix values\n",
    "# cm = confusion_matrix.matrix\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22eb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install \"ultralytics>=8.1.0\" \"torch>=1.8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668503ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b35337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained YOLOv5 model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random subset of 100 samples to add predictions to\n",
    "predictions_view_yolo = train_dataset.take(10, seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for i in train_dataset.info['categories']:\n",
    "    classes.append(i['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a050b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from index to class names\n",
    "id_to_name = {i: name for i, name in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d13ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add predictions to samples\n",
    "with fo.ProgressBar() as progress_bar:\n",
    "    for data_sample in progress_bar(predictions_view_yolo):\n",
    "        # Load image\n",
    "        img = Image.open(data_sample.filepath).convert(\"RGB\")\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        # Perform inference\n",
    "        results = model(img_np)\n",
    "        pred_boxes = results.xyxy[0].cpu().numpy()\n",
    "        pred_labels = results.xyxy[0][:, 5].cpu().numpy().astype(int)  # Get class indices\n",
    "        pred_scores = results.xyxy[0][:, 4].cpu().numpy()\n",
    "\n",
    "        # Convert detections\n",
    "        detections = []\n",
    "        for pred_box, pred_label, pred_score in zip(pred_boxes, pred_labels, pred_scores):\n",
    "            if pred_score >= 0.80:\n",
    "                # Convert to [top-left-x, top-left-y, width, height]\n",
    "                x1, y1, x2, y2 = pred_box[:4]\n",
    "                width, height = img_np.shape[1], img_np.shape[0]\n",
    "                relative_box = [x1 / width, y1 / height, (x2 - x1) / width, (y2 - y1) / height]\n",
    "\n",
    "                # Map the label index to the class name\n",
    "                class_name = id_to_name.get(pred_label, \"unknown\")\n",
    "                detections.append(fo.Detection(\n",
    "                    label=class_name,\n",
    "                    bounding_box=relative_box,\n",
    "                    confidence=pred_score\n",
    "                ))\n",
    "\n",
    "        # Save predictions to dataset\n",
    "        data_sample[\"yolov5\"] = fo.Detections(detections=detections)\n",
    "        data_sample.save()\n",
    "\n",
    "print(\"Finished adding predictions\")\n",
    "\n",
    "# Launch the FiftyOne app to visualize the dataset\n",
    "session = fo.launch_app(predictions_view_yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9331b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using a different evaluation key\n",
    "results = fo.evaluate_detections(\n",
    "    predictions_view_yolo,\n",
    "    \"yolov5\",\n",
    "    eval_key=\"eval_predictions\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    iou=0.5,  # IoU threshold\n",
    "    compute_mAP=True,  # Compute mAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.mAP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List evaluations you've run on a dataset\n",
    "predictions_view_yolo.list_evaluations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4946a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about an evaluation\n",
    "print(predictions_view_yolo.get_evaluation_info(\"eval_predictions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the FiftyOne app to visualize the dataset\n",
    "session = fo.launch_app(predictions_view_yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ab8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 10 most common classes in the dataset\n",
    "counts = predictions_view_yolo.count_values(\"ground_truth.detections.label\")\n",
    "classes = sorted(counts, key=counts.get, reverse=True)[:10]\n",
    "\n",
    "# Print a classification report for the top-10 classes\n",
    "evaluation_results.print_report(classes=classes)\n",
    "\n",
    "# Print some statistics about the total TP/FP/FN counts\n",
    "print(\"TP: %d\" % predictions_view_yolo.sum(\"eval_predictions_tp\"))\n",
    "print(\"FP: %d\" % predictions_view_yolo.sum(\"eval_predictions_fp\"))\n",
    "print(\"FN: %d\" % predictions_view_yolo.sum(\"eval_predictions_fn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda22ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~for phase-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c583ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomCocoDataset(Dataset):\n",
    "#     def __init__(self, dataset, classes, transform=None):\n",
    "#         self.dataset = dataset\n",
    "#         self.transform = transform\n",
    "#         self.class_name_to_id = {name: idx for idx, name in enumerate(classes)}\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sample = self.dataset[idx]\n",
    "#         image_path = sample['filepath']\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         image = np.array(image)  # Convert to numpy array for albumentations\n",
    "\n",
    "#         boxes = []\n",
    "#         labels = []\n",
    "#         for detection in sample.ground_truth.detections:\n",
    "#             box = detection.bounding_box\n",
    "#             width, height = sample.metadata.width, sample.metadata.height\n",
    "#             x1, y1, x2, y2 = box[0] * width, box[1] * height, (box[0] + box[2]) * width, (box[1] + box[3]) * height\n",
    "#             boxes.append([x1, y1, x2, y2])\n",
    "#             labels.append(self.class_name_to_id[detection.label])\n",
    "\n",
    "#         # Prepare data dictionary for transformation\n",
    "#         data_dict = {\n",
    "#             'image': image,\n",
    "#             'bboxes': boxes,\n",
    "#             'class_labels': labels\n",
    "#         }\n",
    "\n",
    "#         # Apply transformations\n",
    "#         transformed = self.transform(**data_dict)\n",
    "#         transformed_image = transformed['image']\n",
    "#         transformed_boxes = transformed['bboxes']\n",
    "#         transformed_labels = transformed['class_labels']\n",
    "\n",
    "#         # Convert boxes and labels to tensor format\n",
    "#         target = {\n",
    "#             \"boxes\": torch.tensor(transformed_boxes, dtype=torch.float32),\n",
    "#             \"labels\": torch.tensor(transformed_labels, dtype=torch.int64)\n",
    "#         }\n",
    "\n",
    "#         # Return transformed image, target, filepath, and original ground_truth\n",
    "#         return transformed_image, target, sample.filepath, sample.ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the transformed dataset\n",
    "# transformed_dataset = CustomCocoDataset(train_dataset, classes, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c19ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c176b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
